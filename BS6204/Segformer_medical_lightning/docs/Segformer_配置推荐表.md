# ğŸ›ï¸ åŒ»å­¦å›¾åƒåˆ†å‰² - GPUé…ç½®æ¨èè¡¨

## ğŸ“Š æ ¹æ®GPUæ˜¾å­˜é€‰æ‹©æœ€ä½³é…ç½®

### ğŸ”¥ é«˜æ€§èƒ½GPUï¼ˆ16GB+ï¼‰

**é€‚ç”¨GPU**ï¼šRTX 4090, A100, RTX 3090, V100

```python
@dataclass
class TrainingConfig:
    BATCH_SIZE: int = 32              # â¬†ï¸ å¤§æ‰¹æ¬¡
    MODEL_NAME: str = "nvidia/segformer-b5-finetuned-ade-512-512"  # æœ€å¤§æ¨¡å‹
    NUM_EPOCHS: int = 100
    INIT_LR: float = 3e-4
    USE_SCHEDULER: bool = True
```

**é¢„æœŸè¡¨ç°**ï¼š
- è®­ç»ƒæ—¶é—´: ~2-3å°æ—¶ï¼ˆ100 epochsï¼‰
- éªŒè¯F1: 0.94-0.96
- å•epoch: ~1-2åˆ†é’Ÿ

---

### âš¡ ä¸»æµGPUï¼ˆ10-12GBï¼‰

**é€‚ç”¨GPU**ï¼šRTX 3060 (12GB), RTX 3080 (10GB), RTX 4070

```python
@dataclass
class TrainingConfig:
    BATCH_SIZE: int = 8               # âœ… æ¨èé…ç½®
    MODEL_NAME: str = "nvidia/segformer-b4-finetuned-ade-512-512"  # é»˜è®¤
    NUM_EPOCHS: int = 100
    INIT_LR: float = 3e-4
    USE_SCHEDULER: bool = True
```

**é¢„æœŸè¡¨ç°**ï¼š
- è®­ç»ƒæ—¶é—´: ~5-7å°æ—¶ï¼ˆ100 epochsï¼‰
- éªŒè¯F1: 0.93-0.95
- å•epoch: ~3-4åˆ†é’Ÿ

---

### ğŸ’» å…¥é—¨çº§GPUï¼ˆ6-8GBï¼‰

**é€‚ç”¨GPU**ï¼šRTX 3050, RTX 2060, GTX 1660 Ti

```python
@dataclass
class TrainingConfig:
    BATCH_SIZE: int = 4               # â¬‡ï¸ å‡å°æ‰¹æ¬¡
    MODEL_NAME: str = "nvidia/segformer-b2-finetuned-ade-512-512"  # ä¸­ç­‰æ¨¡å‹
    NUM_EPOCHS: int = 100
    INIT_LR: float = 3e-4
    USE_SCHEDULER: bool = True

# é¢å¤–ä¼˜åŒ–
@dataclass(frozen=True)
class DatasetConfig:
    IMAGE_SIZE: tuple[int,int] = (256, 256)  # å‡å°å›¾åƒå°ºå¯¸
```

**é¢„æœŸè¡¨ç°**ï¼š
- è®­ç»ƒæ—¶é—´: ~8-12å°æ—¶ï¼ˆ100 epochsï¼‰
- éªŒè¯F1: 0.91-0.93
- å•epoch: ~5-7åˆ†é’Ÿ

---

### ğŸ¯ ä½æ˜¾å­˜GPUï¼ˆ4GBï¼‰

**é€‚ç”¨GPU**ï¼šGTX 1650, RTX 3050 (4GB), MX450

```python
@dataclass
class TrainingConfig:
    BATCH_SIZE: int = 2               # âš ï¸ æœ€å°æ‰¹æ¬¡
    MODEL_NAME: str = "nvidia/segformer-b0-finetuned-ade-512-512"  # æœ€å°æ¨¡å‹
    NUM_EPOCHS: int = 50              # å‡å°‘è®­ç»ƒè½®æ•°
    INIT_LR: float = 2e-4
    USE_SCHEDULER: bool = False

@dataclass(frozen=True)
class DatasetConfig:
    IMAGE_SIZE: tuple[int,int] = (224, 224)  # è¿›ä¸€æ­¥å‡å°
```

**é¢„æœŸè¡¨ç°**ï¼š
- è®­ç»ƒæ—¶é—´: ~6-8å°æ—¶ï¼ˆ50 epochsï¼‰
- éªŒè¯F1: 0.88-0.91
- å•epoch: ~7-10åˆ†é’Ÿ
- âš ï¸ æ€§èƒ½ä¼šæœ‰æ‰€ä¸‹é™

**é¢å¤–ä¼˜åŒ–æŠ€å·§**ï¼š
```python
# å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡ï¼‰
trainer = pl.Trainer(
    accumulate_grad_batches=4,  # ç´¯ç§¯4ä¸ªæ‰¹æ¬¡
    # ... å…¶ä»–å‚æ•°
)
```

---

### ğŸŒ CPUè®­ç»ƒï¼ˆä¸æ¨èï¼‰

å¦‚æœæ²¡æœ‰GPUï¼Œå¯ä»¥ä½¿ç”¨CPUï¼Œä½†**éå¸¸æ…¢**ï¼š

```python
@dataclass
class TrainingConfig:
    BATCH_SIZE: int = 1               # å•æ ·æœ¬
    MODEL_NAME: str = "nvidia/segformer-b0-finetuned-ade-512-512"
    NUM_EPOCHS: int = 5               # åªè®­ç»ƒ5è½®æµ‹è¯•
    NUM_WORKERS: int = 4              # å¤šçº¿ç¨‹åŠ è½½

# è®­ç»ƒå™¨é…ç½®
trainer = pl.Trainer(
    accelerator="cpu",
    devices=1,
    max_epochs=5,
    precision=32,  # CPUä¸æ”¯æŒæ··åˆç²¾åº¦
)
```

**é¢„æœŸè¡¨ç°**ï¼š
- è®­ç»ƒæ—¶é—´: ~50-70å°æ—¶ï¼ˆ100 epochsï¼‰ğŸ˜±
- å•epoch: ~30-40åˆ†é’Ÿ
- **å»ºè®®**ï¼šä»…ç”¨äºä»£ç æµ‹è¯•ï¼Œä¸å»ºè®®å®Œæ•´è®­ç»ƒ

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”è¡¨

| GPUå‹å· | æ˜¾å­˜ | æ¨èBatch | æ¨èæ¨¡å‹ | å•Epoch | 100 Epochs | é¢„æœŸF1 |
|---------|------|-----------|----------|---------|------------|--------|
| RTX 4090 | 24GB | 32 | b5 | 1-2åˆ†é’Ÿ | 2-3å°æ—¶ | 0.95-0.96 |
| RTX 3090 | 24GB | 24 | b5 | 1-2åˆ†é’Ÿ | 2-3å°æ—¶ | 0.95-0.96 |
| RTX 3080 | 10GB | 8 | b4 | 2-3åˆ†é’Ÿ | 3-5å°æ—¶ | 0.93-0.95 |
| **RTX 3060** | **12GB** | **8** | **b4** | **3-4åˆ†é’Ÿ** | **5-7å°æ—¶** | **0.93-0.95** |
| RTX 3050 | 8GB | 4 | b2 | 5-7åˆ†é’Ÿ | 8-12å°æ—¶ | 0.91-0.93 |
| GTX 1650 | 4GB | 2 | b0 | 7-10åˆ†é’Ÿ | 12-17å°æ—¶ | 0.88-0.91 |
| CPU | N/A | 1 | b0 | 30-40åˆ†é’Ÿ | 50-70å°æ—¶ | 0.85-0.88 |

---

## ğŸ¯ æ¨¡å‹é€‰æ‹©æŒ‡å—

### SegFormeræ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | æ˜¾å­˜å ç”¨ | é€Ÿåº¦ | ç²¾åº¦ | é€‚ç”¨åœºæ™¯ |
|------|--------|----------|------|------|----------|
| **b0** | 4M | ~4GB | âš¡âš¡âš¡âš¡âš¡ | â­â­â­ | å¿«é€Ÿå®éªŒã€ä½æ˜¾å­˜ |
| **b1** | 14M | ~6GB | âš¡âš¡âš¡âš¡ | â­â­â­â­ | å¹³è¡¡æ€§èƒ½ |
| **b2** | 28M | ~8GB | âš¡âš¡âš¡ | â­â­â­â­ | å…¥é—¨çº§GPU |
| **b3** | 47M | ~10GB | âš¡âš¡ | â­â­â­â­â­ | ä¸»æµGPU |
| **b4** | 64M | ~12GB | âš¡âš¡ | â­â­â­â­â­ | **é»˜è®¤æ¨è** |
| **b5** | 84M | ~16GB | âš¡ | â­â­â­â­â­ | é«˜æ€§èƒ½GPUã€ç”Ÿäº§ç¯å¢ƒ |

---

## ğŸ’¡ ä¼˜åŒ–æŠ€å·§

### æŠ€å·§1: æ¢¯åº¦ç´¯ç§¯ï¼ˆå°æ˜¾å­˜ç¦éŸ³ï¼‰

æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡è®­ç»ƒæ•ˆæœï¼š

```python
# é…ç½®
TrainingConfig.BATCH_SIZE = 4

# Trainerè®¾ç½®
trainer = pl.Trainer(
    accumulate_grad_batches=4,  # 4 Ã— 4 = æœ‰æ•ˆæ‰¹æ¬¡16
    # ...
)
```

**åŸç†**ï¼šç´¯ç§¯4ä¸ªå°æ‰¹æ¬¡çš„æ¢¯åº¦ï¼Œç„¶åä¸€æ¬¡æ€§æ›´æ–°ã€‚

### æŠ€å·§2: æ··åˆç²¾åº¦è®­ç»ƒï¼ˆ2å€åŠ é€Ÿï¼‰

```python
trainer = pl.Trainer(
    precision="16-mixed",  # è‡ªåŠ¨ä½¿ç”¨FP16
    # ...
)
```

**æ•ˆæœ**ï¼š
- é€Ÿåº¦æå‡: 1.5-2å€
- æ˜¾å­˜èŠ‚çœ: ~30%
- ç²¾åº¦æŸå¤±: å¯å¿½ç•¥

### æŠ€å·§3: æ¸è¿›å¼è°ƒæ•´

ä»å°æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥å‡çº§ï¼š

```
ç¬¬1è½®: b0æ¨¡å‹ + å°å›¾åƒ(224) â†’ éªŒè¯ç¯å¢ƒ
ç¬¬2è½®: b2æ¨¡å‹ + ä¸­å›¾åƒ(256) â†’ åˆæ­¥è®­ç»ƒ
ç¬¬3è½®: b4æ¨¡å‹ + æ ‡å‡†å›¾åƒ(288) â†’ å®Œæ•´è®­ç»ƒ
```

### æŠ€å·§4: æ•°æ®åŠ è½½ä¼˜åŒ–

```python
# Windows
TrainingConfig.NUM_WORKERS = 0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜

# Linux/Mac
TrainingConfig.NUM_WORKERS = 4  # å¹¶è¡ŒåŠ è½½

# å¯ç”¨pin_memoryï¼ˆGPUè®­ç»ƒæ¨èï¼‰
data_module = MedicalSegmentationDataModule(
    pin_memory=True,  # åŠ é€ŸCPUâ†’GPUä¼ è¾“
    # ...
)
```

### æŠ€å·§5: å­¦ä¹ ç‡æŸ¥æ‰¾

è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜å­¦ä¹ ç‡ï¼š

```python
from lightning.pytorch.tuner import Tuner

tuner = Tuner(trainer)
lr_finder = tuner.lr_find(model, data_module)

# æŸ¥çœ‹å»ºè®®çš„å­¦ä¹ ç‡
print(f"å»ºè®®å­¦ä¹ ç‡: {lr_finder.suggestion()}")

# å¯è§†åŒ–
fig = lr_finder.plot(suggest=True)
fig.show()
```

---

## ğŸ“Š æ˜¾å­˜å ç”¨ä¼°ç®—

### æ˜¾å­˜åˆ†é…

```
æ€»æ˜¾å­˜ = æ¨¡å‹å‚æ•° + ä¼˜åŒ–å™¨çŠ¶æ€ + æ¿€æ´»å€¼ + æ‰¹æ¬¡æ•°æ®

ç¤ºä¾‹ï¼ˆb4æ¨¡å‹ï¼Œbatch=8ï¼‰ï¼š
- æ¨¡å‹å‚æ•°: ~250MB
- ä¼˜åŒ–å™¨(AdamW): ~500MB
- æ¿€æ´»å€¼: ~3GB
- æ‰¹æ¬¡æ•°æ®: ~200MB
- å…¶ä»–å¼€é”€: ~500MB
------------------------
æ€»è®¡: ~4.5GB

+ æ··åˆç²¾åº¦: å‡å°‘30% â†’ ~3.2GB
+ æ¢¯åº¦ç´¯ç§¯: å‡ ä¹ä¸å¢åŠ 
```

### å®æµ‹æ˜¾å­˜å ç”¨ï¼ˆNVIDIA-SMIï¼‰

| é…ç½® | æ¨¡å‹ | Batch | å›¾åƒå¤§å° | å®é™…å ç”¨ |
|------|------|-------|----------|----------|
| æœ€å° | b0 | 2 | 224Ã—224 | ~2.5GB |
| å…¥é—¨ | b2 | 4 | 256Ã—256 | ~4.5GB |
| æ¨è | b4 | 8 | 288Ã—288 | ~7.5GB |
| é«˜æ€§èƒ½ | b5 | 16 | 288Ã—288 | ~12GB |

---

## ğŸ”§ è°ƒè¯•æŠ€å·§

### æ‰¾åˆ°æœ€å¤§æ‰¹æ¬¡å¤§å°

ä½¿ç”¨äºŒåˆ†æœç´¢æ³•ï¼š

```python
# æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
batch_sizes = [32, 16, 8, 4, 2]

for bs in batch_sizes:
    try:
        TrainingConfig.BATCH_SIZE = bs
        # å°è¯•è®­ç»ƒä¸€ä¸ªepoch
        trainer.fit(model, data_module, max_epochs=1)
        print(f"âœ“ Batch {bs} æˆåŠŸ")
        break
    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"âœ— Batch {bs} æ˜¾å­˜ä¸è¶³")
            torch.cuda.empty_cache()
        else:
            raise e
```

### ç›‘æ§æ˜¾å­˜ä½¿ç”¨

```python
import torch

def print_gpu_utilization():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"å·²åˆ†é…: {allocated:.2f}GB")
        print(f"å·²ä¿ç•™: {reserved:.2f}GB")

# åœ¨è®­ç»ƒå‰åè°ƒç”¨
print_gpu_utilization()
```

---

## ğŸ“ å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆæˆ‘çš„GPUæ¯”è¡¨æ ¼æ…¢ï¼Ÿ

å¯èƒ½åŸå› ï¼š
1. âœ… CPUç“¶é¢ˆ â†’ å¢åŠ `NUM_WORKERS`
2. âœ… ç£ç›˜I/Oæ…¢ â†’ ä½¿ç”¨SSD
3. âœ… æ¸©åº¦è¿‡é«˜ â†’ æ£€æŸ¥æ•£çƒ­
4. âœ… å…¶ä»–ç¨‹åºå ç”¨GPU â†’ å…³é—­å…¶ä»–ç¨‹åº

### Q: å¦‚ä½•åœ¨Colabä¸Šè¿è¡Œï¼Ÿ

```python
# 1. æ£€æŸ¥GPU
!nvidia-smi

# 2. ä¿®æ”¹é…ç½®
TrainingConfig.BATCH_SIZE = 16  # T4 GPU
TrainingConfig.NUM_WORKERS = 2

# 3. æŒ‚è½½Google Driveä¿å­˜æ¨¡å‹
from google.colab import drive
drive.mount('/content/drive')
```

### Q: å¤šGPUè®­ç»ƒå¦‚ä½•é…ç½®ï¼Ÿ

```python
# è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰GPU
trainer = pl.Trainer(
    accelerator="gpu",
    devices=-1,  # ä½¿ç”¨æ‰€æœ‰GPU
    strategy="ddp",  # åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
    # ...
)

# æ‰¹æ¬¡å¤§å°ä¼šè‡ªåŠ¨è°ƒæ•´
# å®é™…æ‰¹æ¬¡ = BATCH_SIZE Ã— GPUæ•°é‡
```

---

## âœ… é…ç½®æ£€æŸ¥æ¸…å•

åœ¨å¼€å§‹è®­ç»ƒå‰ï¼Œç¡®è®¤ä»¥ä¸‹å‡ ç‚¹ï¼š

- [ ] å·²å®‰è£…æ­£ç¡®çš„PyTorchï¼ˆCUDAç‰ˆæœ¬åŒ¹é…ï¼‰
- [ ] æ˜¾å­˜è¶³å¤Ÿï¼ˆè‡³å°‘æ¯”æ¨èé…ç½®å¤š1GBä½™é‡ï¼‰
- [ ] æ•°æ®é›†å·²ä¸‹è½½å®Œæˆ
- [ ] æ‰¹æ¬¡å¤§å°å’Œæ¨¡å‹å¤§å°å·²æ ¹æ®GPUè°ƒæ•´
- [ ] å¯ç”¨äº†æ··åˆç²¾åº¦è®­ç»ƒï¼ˆ`precision="16-mixed"`ï¼‰
- [ ] å…³é—­äº†å…¶ä»–å ç”¨GPUçš„ç¨‹åº
- [ ] è®¾ç½®äº†åˆé€‚çš„`NUM_WORKERS`

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•

ä½¿ç”¨è¿™æ®µä»£ç æµ‹è¯•æ‚¨çš„é…ç½®ï¼š

```python
import time
import torch

def benchmark_training(model, data_module, batch_size, num_steps=100):
    model.train()
    model.to("cuda")

    data_module.setup()
    loader = data_module.train_dataloader()

    start_time = time.time()

    for i, (images, masks) in enumerate(loader):
        if i >= num_steps:
            break

        images = images.to("cuda")
        masks = masks.to("cuda")

        # å‰å‘ä¼ æ’­
        logits = model(images)
        loss = dice_coef_loss(logits, masks, num_classes=4)

        # åå‘ä¼ æ’­
        loss.backward()

    elapsed = time.time() - start_time
    throughput = num_steps * batch_size / elapsed

    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"å¤„ç†é€Ÿåº¦: {throughput:.2f} å›¾åƒ/ç§’")
    print(f"å•æ‰¹æ¬¡æ—¶é—´: {elapsed/num_steps*1000:.2f}ms")

# è¿è¡Œæµ‹è¯•
benchmark_training(model, data_module, TrainingConfig.BATCH_SIZE)
```

---

**ğŸ¯ é€‰æ‹©åˆé€‚çš„é…ç½®ï¼Œå¼€å§‹æ‚¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²ä¹‹æ—…å§ï¼**
